{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF969 Coursework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J11M-BL9MMTH"
      },
      "source": [
        "## This body of work will use Machine Learning techniques to forecast stock returns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv_1keT1MYrL"
      },
      "source": [
        "#import libraries \n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas_profiling\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import sklearn\n",
        " \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import make_scorer, confusion_matrix\n",
        "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgK4IZLxEMUJ"
      },
      "source": [
        "# link to yahoo finance for Apple historical data - 03/01/2000 - 23/04/2021\n",
        "# https://uk.finance.yahoo.com/quote/AAPL/history?p=AAPL"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "4rYS1WgWN-6O",
        "outputId": "f9511478-cc64-4a37-de2b-43b1e937f485"
      },
      "source": [
        "#import file (data is downloaded from yahpp finaance website, it is daily data for apple stock from 03/01/2000 until the day this piece is produced.)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b1830732-f7fc-43b1-91ee-a8a9245e626e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b1830732-f7fc-43b1-91ee-a8a9245e626e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving AAPL (3).csv to AAPL (3).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "t_7dULE7OYWM",
        "outputId": "84075671-5d07-41f6-fe74-bc651cef3cce"
      },
      "source": [
        "#import data (Apple daily stocks returns for yahoo finance)\n",
        "df = pd.read_csv('/content/AAPL (3).csv')\n",
        "df.head(n=10)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.999442</td>\n",
              "      <td>0.860883</td>\n",
              "      <td>535796800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0.966518</td>\n",
              "      <td>0.987723</td>\n",
              "      <td>0.903460</td>\n",
              "      <td>0.915179</td>\n",
              "      <td>0.788302</td>\n",
              "      <td>512377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0.926339</td>\n",
              "      <td>0.987165</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.799837</td>\n",
              "      <td>778321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>0.947545</td>\n",
              "      <td>0.955357</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.730621</td>\n",
              "      <td>767972800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>0.861607</td>\n",
              "      <td>0.901786</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>0.888393</td>\n",
              "      <td>0.765229</td>\n",
              "      <td>460734400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2000-01-10</td>\n",
              "      <td>0.910714</td>\n",
              "      <td>0.912946</td>\n",
              "      <td>0.845982</td>\n",
              "      <td>0.872768</td>\n",
              "      <td>0.751771</td>\n",
              "      <td>505064000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2000-01-11</td>\n",
              "      <td>0.856585</td>\n",
              "      <td>0.887277</td>\n",
              "      <td>0.808036</td>\n",
              "      <td>0.828125</td>\n",
              "      <td>0.713317</td>\n",
              "      <td>441548800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2000-01-12</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>0.772321</td>\n",
              "      <td>0.778460</td>\n",
              "      <td>0.670537</td>\n",
              "      <td>976068800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2000-01-13</td>\n",
              "      <td>0.843610</td>\n",
              "      <td>0.881696</td>\n",
              "      <td>0.825893</td>\n",
              "      <td>0.863839</td>\n",
              "      <td>0.744079</td>\n",
              "      <td>1032684800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2000-01-14</td>\n",
              "      <td>0.892857</td>\n",
              "      <td>0.912946</td>\n",
              "      <td>0.887277</td>\n",
              "      <td>0.896763</td>\n",
              "      <td>0.772439</td>\n",
              "      <td>390376000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close  Adj Close      Volume\n",
              "0  2000-01-03  0.936384  1.004464  0.907924  0.999442   0.860883   535796800\n",
              "1  2000-01-04  0.966518  0.987723  0.903460  0.915179   0.788302   512377600\n",
              "2  2000-01-05  0.926339  0.987165  0.919643  0.928571   0.799837   778321600\n",
              "3  2000-01-06  0.947545  0.955357  0.848214  0.848214   0.730621   767972800\n",
              "4  2000-01-07  0.861607  0.901786  0.852679  0.888393   0.765229   460734400\n",
              "5  2000-01-10  0.910714  0.912946  0.845982  0.872768   0.751771   505064000\n",
              "6  2000-01-11  0.856585  0.887277  0.808036  0.828125   0.713317   441548800\n",
              "7  2000-01-12  0.848214  0.852679  0.772321  0.778460   0.670537   976068800\n",
              "8  2000-01-13  0.843610  0.881696  0.825893  0.863839   0.744079  1032684800\n",
              "9  2000-01-14  0.892857  0.912946  0.887277  0.896763   0.772439   390376000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "3GE3IUJ_ImwN",
        "outputId": "fc2d4911-5912-4c7b-9c88-cc415b68b587"
      },
      "source": [
        "#add column of log returns for Apple stock using closing prices \n",
        "df['log return'] = np.log(df.Close) - np.log(df.Close.shift(1))\n",
        "df"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>log return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.999442</td>\n",
              "      <td>0.860883</td>\n",
              "      <td>535796800</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0.966518</td>\n",
              "      <td>0.987723</td>\n",
              "      <td>0.903460</td>\n",
              "      <td>0.915179</td>\n",
              "      <td>0.788302</td>\n",
              "      <td>512377600</td>\n",
              "      <td>-0.088077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0.926339</td>\n",
              "      <td>0.987165</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.799837</td>\n",
              "      <td>778321600</td>\n",
              "      <td>0.014527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>0.947545</td>\n",
              "      <td>0.955357</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.730621</td>\n",
              "      <td>767972800</td>\n",
              "      <td>-0.090514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>0.861607</td>\n",
              "      <td>0.901786</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>0.888393</td>\n",
              "      <td>0.765229</td>\n",
              "      <td>460734400</td>\n",
              "      <td>0.046281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5356</th>\n",
              "      <td>2021-04-19</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>135.470001</td>\n",
              "      <td>133.339996</td>\n",
              "      <td>134.839996</td>\n",
              "      <td>134.839996</td>\n",
              "      <td>94264200</td>\n",
              "      <td>0.005056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5357</th>\n",
              "      <td>2021-04-20</td>\n",
              "      <td>135.020004</td>\n",
              "      <td>135.529999</td>\n",
              "      <td>131.809998</td>\n",
              "      <td>133.110001</td>\n",
              "      <td>133.110001</td>\n",
              "      <td>94812300</td>\n",
              "      <td>-0.012913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5358</th>\n",
              "      <td>2021-04-21</td>\n",
              "      <td>132.360001</td>\n",
              "      <td>133.750000</td>\n",
              "      <td>131.300003</td>\n",
              "      <td>133.500000</td>\n",
              "      <td>133.500000</td>\n",
              "      <td>68847100</td>\n",
              "      <td>0.002926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5359</th>\n",
              "      <td>2021-04-22</td>\n",
              "      <td>133.039993</td>\n",
              "      <td>134.149994</td>\n",
              "      <td>131.410004</td>\n",
              "      <td>131.940002</td>\n",
              "      <td>131.940002</td>\n",
              "      <td>84566500</td>\n",
              "      <td>-0.011754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5360</th>\n",
              "      <td>2021-04-23</td>\n",
              "      <td>132.160004</td>\n",
              "      <td>135.119995</td>\n",
              "      <td>132.160004</td>\n",
              "      <td>134.320007</td>\n",
              "      <td>134.320007</td>\n",
              "      <td>78657500</td>\n",
              "      <td>0.017878</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5361 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date        Open        High  ...   Adj Close     Volume  log return\n",
              "0     2000-01-03    0.936384    1.004464  ...    0.860883  535796800         NaN\n",
              "1     2000-01-04    0.966518    0.987723  ...    0.788302  512377600   -0.088077\n",
              "2     2000-01-05    0.926339    0.987165  ...    0.799837  778321600    0.014527\n",
              "3     2000-01-06    0.947545    0.955357  ...    0.730621  767972800   -0.090514\n",
              "4     2000-01-07    0.861607    0.901786  ...    0.765229  460734400    0.046281\n",
              "...          ...         ...         ...  ...         ...        ...         ...\n",
              "5356  2021-04-19  133.509995  135.470001  ...  134.839996   94264200    0.005056\n",
              "5357  2021-04-20  135.020004  135.529999  ...  133.110001   94812300   -0.012913\n",
              "5358  2021-04-21  132.360001  133.750000  ...  133.500000   68847100    0.002926\n",
              "5359  2021-04-22  133.039993  134.149994  ...  131.940002   84566500   -0.011754\n",
              "5360  2021-04-23  132.160004  135.119995  ...  134.320007   78657500    0.017878\n",
              "\n",
              "[5361 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "pAeslnRMP44U",
        "outputId": "e7335ba5-a750-44ce-c2a4-5f3dfc14fddd"
      },
      "source": [
        "#change values of target variable to 1 or 0, 0 for negtaive daily price move, 1 for positive daily price move \n",
        "df['log return'] = [0 if i <=0 else 1 for i in df['log return']]\n",
        "df"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>log return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.999442</td>\n",
              "      <td>0.860883</td>\n",
              "      <td>535796800</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0.966518</td>\n",
              "      <td>0.987723</td>\n",
              "      <td>0.903460</td>\n",
              "      <td>0.915179</td>\n",
              "      <td>0.788302</td>\n",
              "      <td>512377600</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0.926339</td>\n",
              "      <td>0.987165</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.799837</td>\n",
              "      <td>778321600</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>0.947545</td>\n",
              "      <td>0.955357</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.730621</td>\n",
              "      <td>767972800</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>0.861607</td>\n",
              "      <td>0.901786</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>0.888393</td>\n",
              "      <td>0.765229</td>\n",
              "      <td>460734400</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5356</th>\n",
              "      <td>2021-04-19</td>\n",
              "      <td>133.509995</td>\n",
              "      <td>135.470001</td>\n",
              "      <td>133.339996</td>\n",
              "      <td>134.839996</td>\n",
              "      <td>134.839996</td>\n",
              "      <td>94264200</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5357</th>\n",
              "      <td>2021-04-20</td>\n",
              "      <td>135.020004</td>\n",
              "      <td>135.529999</td>\n",
              "      <td>131.809998</td>\n",
              "      <td>133.110001</td>\n",
              "      <td>133.110001</td>\n",
              "      <td>94812300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5358</th>\n",
              "      <td>2021-04-21</td>\n",
              "      <td>132.360001</td>\n",
              "      <td>133.750000</td>\n",
              "      <td>131.300003</td>\n",
              "      <td>133.500000</td>\n",
              "      <td>133.500000</td>\n",
              "      <td>68847100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5359</th>\n",
              "      <td>2021-04-22</td>\n",
              "      <td>133.039993</td>\n",
              "      <td>134.149994</td>\n",
              "      <td>131.410004</td>\n",
              "      <td>131.940002</td>\n",
              "      <td>131.940002</td>\n",
              "      <td>84566500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5360</th>\n",
              "      <td>2021-04-23</td>\n",
              "      <td>132.160004</td>\n",
              "      <td>135.119995</td>\n",
              "      <td>132.160004</td>\n",
              "      <td>134.320007</td>\n",
              "      <td>134.320007</td>\n",
              "      <td>78657500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5361 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date        Open        High  ...   Adj Close     Volume  log return\n",
              "0     2000-01-03    0.936384    1.004464  ...    0.860883  535796800           1\n",
              "1     2000-01-04    0.966518    0.987723  ...    0.788302  512377600           0\n",
              "2     2000-01-05    0.926339    0.987165  ...    0.799837  778321600           1\n",
              "3     2000-01-06    0.947545    0.955357  ...    0.730621  767972800           0\n",
              "4     2000-01-07    0.861607    0.901786  ...    0.765229  460734400           1\n",
              "...          ...         ...         ...  ...         ...        ...         ...\n",
              "5356  2021-04-19  133.509995  135.470001  ...  134.839996   94264200           1\n",
              "5357  2021-04-20  135.020004  135.529999  ...  133.110001   94812300           0\n",
              "5358  2021-04-21  132.360001  133.750000  ...  133.500000   68847100           1\n",
              "5359  2021-04-22  133.039993  134.149994  ...  131.940002   84566500           0\n",
              "5360  2021-04-23  132.160004  135.119995  ...  134.320007   78657500           1\n",
              "\n",
              "[5361 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fWUaQRZTT9T",
        "outputId": "2314ab56-16ce-4956-8518-e79a467c64ce"
      },
      "source": [
        "#check nan's\n",
        "df.isna().sum()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date          0\n",
              "Open          0\n",
              "High          0\n",
              "Low           0\n",
              "Close         0\n",
              "Adj Close     0\n",
              "Volume        0\n",
              "log return    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "By6RTds5TVta",
        "outputId": "f80e7397-ea77-4b45-c0f1-078a8c96a7c3"
      },
      "source": [
        "# plot distribution of numerical data in df and observe (tragey varibale is fairly balanced which is important, explanatory variables are negatively skewed which is expected for financial data, we general ignore this)\n",
        "df.hist()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe00cbbd0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe00c1cd0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe011efd0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe017ee90>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fe003c210>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fa61c8790>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7f1fa617ed10>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fa61412d0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7f1fa614b390>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQTcaz7BTfSD",
        "outputId": "0990bd28-3736-42ad-e5f2-422f8c8719a8"
      },
      "source": [
        "## check types are sufficient\n",
        "df.dtypes"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Date           object\n",
              "Open          float64\n",
              "High          float64\n",
              "Low           float64\n",
              "Close         float64\n",
              "Adj Close     float64\n",
              "Volume          int64\n",
              "log return      int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBgjanV8TuV6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0308c4fe-ecc1-434e-cacf-45635a51c0f3"
      },
      "source": [
        "#drop date column\n",
        "df = df.drop([\"Date\"], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>log return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.936384</td>\n",
              "      <td>1.004464</td>\n",
              "      <td>0.907924</td>\n",
              "      <td>0.999442</td>\n",
              "      <td>0.860883</td>\n",
              "      <td>535796800</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.966518</td>\n",
              "      <td>0.987723</td>\n",
              "      <td>0.903460</td>\n",
              "      <td>0.915179</td>\n",
              "      <td>0.788302</td>\n",
              "      <td>512377600</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.926339</td>\n",
              "      <td>0.987165</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.799837</td>\n",
              "      <td>778321600</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.947545</td>\n",
              "      <td>0.955357</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.848214</td>\n",
              "      <td>0.730621</td>\n",
              "      <td>767972800</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.861607</td>\n",
              "      <td>0.901786</td>\n",
              "      <td>0.852679</td>\n",
              "      <td>0.888393</td>\n",
              "      <td>0.765229</td>\n",
              "      <td>460734400</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Open      High       Low     Close  Adj Close     Volume  log return\n",
              "0  0.936384  1.004464  0.907924  0.999442   0.860883  535796800           1\n",
              "1  0.966518  0.987723  0.903460  0.915179   0.788302  512377600           0\n",
              "2  0.926339  0.987165  0.919643  0.928571   0.799837  778321600           1\n",
              "3  0.947545  0.955357  0.848214  0.848214   0.730621  767972800           0\n",
              "4  0.861607  0.901786  0.852679  0.888393   0.765229  460734400           1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIMUMhEZUAeL",
        "outputId": "f5668922-7592-4443-c208-2d5399c85ab2"
      },
      "source": [
        "## extract features \n",
        "features = df.columns\n",
        "print(features)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'log return'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "aJKexPceUSOz",
        "outputId": "4df4394c-b773-4014-9b4f-ac70e98ec460"
      },
      "source": [
        "## Perform feature scaling to standardize remaining numerical values \n",
        "#import standard scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Perform feature scaling \n",
        "standardScaler = StandardScaler()\n",
        "columns_for_ft_scaling = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
        "\n",
        "#apply the fearure scaling operation on the datset using fit_transform() method\n",
        "df[columns_for_ft_scaling] = standardScaler.fit_transform(df[columns_for_ft_scaling])\n",
        "df.head(-1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>log return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.715977</td>\n",
              "      <td>-0.713080</td>\n",
              "      <td>-0.717784</td>\n",
              "      <td>-0.713739</td>\n",
              "      <td>-0.684642</td>\n",
              "      <td>0.244283</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.714864</td>\n",
              "      <td>-0.713691</td>\n",
              "      <td>-0.717951</td>\n",
              "      <td>-0.716854</td>\n",
              "      <td>-0.687338</td>\n",
              "      <td>0.184301</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.716349</td>\n",
              "      <td>-0.713712</td>\n",
              "      <td>-0.717345</td>\n",
              "      <td>-0.716359</td>\n",
              "      <td>-0.686909</td>\n",
              "      <td>0.865443</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.715565</td>\n",
              "      <td>-0.714874</td>\n",
              "      <td>-0.720019</td>\n",
              "      <td>-0.719328</td>\n",
              "      <td>-0.689481</td>\n",
              "      <td>0.838937</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.718741</td>\n",
              "      <td>-0.716831</td>\n",
              "      <td>-0.719852</td>\n",
              "      <td>-0.717843</td>\n",
              "      <td>-0.688195</td>\n",
              "      <td>0.052032</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5355</th>\n",
              "      <td>4.213300</td>\n",
              "      <td>4.169644</td>\n",
              "      <td>4.237367</td>\n",
              "      <td>4.207450</td>\n",
              "      <td>4.267425</td>\n",
              "      <td>-0.910772</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5356</th>\n",
              "      <td>4.184100</td>\n",
              "      <td>4.198868</td>\n",
              "      <td>4.239613</td>\n",
              "      <td>4.232581</td>\n",
              "      <td>4.292686</td>\n",
              "      <td>-0.886580</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5357</th>\n",
              "      <td>4.239912</td>\n",
              "      <td>4.201059</td>\n",
              "      <td>4.182339</td>\n",
              "      <td>4.168646</td>\n",
              "      <td>4.228417</td>\n",
              "      <td>-0.885176</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5358</th>\n",
              "      <td>4.141595</td>\n",
              "      <td>4.136037</td>\n",
              "      <td>4.163248</td>\n",
              "      <td>4.183059</td>\n",
              "      <td>4.242905</td>\n",
              "      <td>-0.951679</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5359</th>\n",
              "      <td>4.166728</td>\n",
              "      <td>4.150649</td>\n",
              "      <td>4.167366</td>\n",
              "      <td>4.125406</td>\n",
              "      <td>4.184951</td>\n",
              "      <td>-0.911418</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5360 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Open      High       Low     Close  Adj Close    Volume  log return\n",
              "0    -0.715977 -0.713080 -0.717784 -0.713739  -0.684642  0.244283           1\n",
              "1    -0.714864 -0.713691 -0.717951 -0.716854  -0.687338  0.184301           0\n",
              "2    -0.716349 -0.713712 -0.717345 -0.716359  -0.686909  0.865443           1\n",
              "3    -0.715565 -0.714874 -0.720019 -0.719328  -0.689481  0.838937           0\n",
              "4    -0.718741 -0.716831 -0.719852 -0.717843  -0.688195  0.052032           1\n",
              "...        ...       ...       ...       ...        ...       ...         ...\n",
              "5355  4.213300  4.169644  4.237367  4.207450   4.267425 -0.910772           0\n",
              "5356  4.184100  4.198868  4.239613  4.232581   4.292686 -0.886580           1\n",
              "5357  4.239912  4.201059  4.182339  4.168646   4.228417 -0.885176           0\n",
              "5358  4.141595  4.136037  4.163248  4.183059   4.242905 -0.951679           1\n",
              "5359  4.166728  4.150649  4.167366  4.125406   4.184951 -0.911418           0\n",
              "\n",
              "[5360 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHjqJuKjUnOz"
      },
      "source": [
        "#set x and y variables \n",
        "x = np.array(df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']])\n",
        "y = np.array(df['log return'])"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA100xNLWNiW"
      },
      "source": [
        "## split into training and test data \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.10, random_state = 42, shuffle=False)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tLAGy-0J4x8"
      },
      "source": [
        "### Will now train and test some ML models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urlS7fK8WXrT",
        "outputId": "af7f0737-ec65-4585-8f55-cb6fbe310735"
      },
      "source": [
        "#Fit logistic regression model \n",
        "logmodel = LogisticRegression(random_state=42)\n",
        "logmodel.fit(x_train,y_train)\n",
        "\n",
        "#predict the results\n",
        "predict_lm = logmodel.predict(x_test)\n",
        "\n",
        "#Calculate accuracy \n",
        "log_model_accuracy = round(metrics.accuracy_score(y_test, predict_lm) * 100, 2)\n",
        "print(\"Logmodel accuracy:\", log_model_accuracy)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logmodel accuracy: 60.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4acyGvvhWZ2l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60af521d-6a54-47b0-a308-014e8fb39523"
      },
      "source": [
        "#Fit a Support Vector Machine Model\n",
        "svm_model = SVC(kernel = 'linear', random_state = 42, probability = True)\n",
        "svm_model.fit(x_train,y_train)\n",
        "\n",
        "#predict results\n",
        "svm_predict = svm_model.predict(x_test)\n",
        "\n",
        "#Calculate accuracy \n",
        "svm_accuracy = round(metrics.accuracy_score(y_test, svm_predict) * 100, 2)\n",
        "print(\" SVC model accuracy:\", svm_accuracy)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " SVC model accuracy: 62.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee8nJBN6Wk8j",
        "outputId": "6360ebd3-17d0-4980-8133-2077d1fe3600"
      },
      "source": [
        "#Fit K-Nearest Neighbour model \n",
        "kmodel = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) #p=2 represents Euclidean distance \n",
        "kmodel.fit(x_train,y_train)\n",
        "\n",
        "#Predict results\n",
        "k_predict = kmodel.predict(x_test)\n",
        "\n",
        "#Calcualte accuracy of the model \n",
        "k_accuracy = round(metrics.accuracy_score(y_test, k_predict) * 100, 2)\n",
        "print(\"K-Nearest Neighbours model:\", k_accuracy) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K-Nearest Neighbours model: 54.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1g5OJtlWo5L",
        "outputId": "633f9b96-7914-4b93-cbba-9dab0cd54fc2"
      },
      "source": [
        "#Fit Decesion Tree model\n",
        "dtmodel = DecisionTreeClassifier(criterion = \"gini\", random_state = 42)\n",
        "dtmodel.fit(x_train,y_train)\n",
        "\n",
        "#Predict results\n",
        "dt_predict = dtmodel.predict(x_test)\n",
        "\n",
        "#Calculate accuracy\n",
        "dt_accuracy = round(metrics.accuracy_score(y_test, dt_predict) * 100, 2)\n",
        "print(\"Decesion Tree model accuracy:\", dt_accuracy)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decesion Tree model accuracy: 58.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PBIy4_gWsli",
        "outputId": "912bb902-4431-4c15-a9aa-745e00783e15"
      },
      "source": [
        "##Fit random forrest\n",
        "rf_classifier = RandomForestClassifier(random_state = 42)\n",
        "rf_classifier.fit(x_train,y_train)\n",
        "\n",
        "#predict results\n",
        "rf_predict = rf_classifier.predict(x_test)\n",
        "\n",
        "#calculate accuracy \n",
        "rf_accuracy = round(metrics.accuracy_score(y_test, rf_predict) * 100, 2)\n",
        "print(\"Random Forest Accuracy:\", rf_accuracy)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 53.63\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmhOMibxWvwi",
        "outputId": "cfc06f15-689c-47ad-c7e4-365f96e54527"
      },
      "source": [
        "\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "gb_clf.fit(x_train,y_train)\n",
        "\n",
        "gb_predict = gb_clf.predict(x_test)\n",
        "\n",
        "gb_accuracy = round(metrics.accuracy_score(y_test, gb_predict) * 100, 2)\n",
        "print(\"GradientBoostingClassifier Accuracy:\", gb_accuracy)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GradientBoostingClassifier Accuracy: 49.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Q0wVs7LXWzaL",
        "outputId": "697b5892-f346-4373-956f-f9e7fd41fa71"
      },
      "source": [
        "#Create table of accuracies \n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model' : ['Logistic Regression', 'Support Vector Machine', 'K_Nearest Neighbours', 'Decesion Tree', 'Random Forest', 'GradientBoostingClassifier'],\n",
        "    'Score' : [log_model_accuracy, svm_accuracy, k_accuracy, dt_accuracy, rf_accuracy, gb_accuracy]\n",
        "})\n",
        "Model_comparison_df = model_comparison.sort_values(by='Score', ascending=False)\n",
        "Model_comparison_df = Model_comparison_df.set_index('Score')\n",
        "Model_comparison_df.reset_index()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Score</th>\n",
              "      <th>Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>62.01</td>\n",
              "      <td>Support Vector Machine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60.34</td>\n",
              "      <td>Logistic Regression</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>58.10</td>\n",
              "      <td>Decesion Tree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54.56</td>\n",
              "      <td>K_Nearest Neighbours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53.63</td>\n",
              "      <td>Random Forest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>49.91</td>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Score                       Model\n",
              "0  62.01      Support Vector Machine\n",
              "1  60.34         Logistic Regression\n",
              "2  58.10               Decesion Tree\n",
              "3  54.56        K_Nearest Neighbours\n",
              "4  53.63               Random Forest\n",
              "5  49.91  GradientBoostingClassifier"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "hd2ycAVcOol3",
        "outputId": "30d2852f-6846-4c69-f6de-734eaa70b572"
      },
      "source": [
        "#Generate confusion matrix for model evauluation, will use model with the highest accuracy \n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "cm = confusion_matrix(y_test, svm_predict)\n",
        "print(cm)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Confusion Matrix', size=16)\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 40 202]\n",
            " [  2 293]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f20002a3e50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VPAoaNvRHGM"
      },
      "source": [
        "## As can be seen by the confusion matrix our best performing model, in terms of accuracy, is poor. The model is predicting that the price will move higher more often than it is, sucessfully capturing 293 True Positives, but miss prediciting 202 False positives. The few times it has predicted a down turn it has been realatively sucessful with 40 True Negatives and only 2 False positives. I believe accuracay for this business problem is the most suitable metric. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g79wJyGXKGmA"
      },
      "source": [
        "### I will now forecast 5 days of positive/negative daily stock returns for Apple stock "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANJP3G8ScNAK",
        "outputId": "1676c133-c636-4b3b-a193-7855adfecb23"
      },
      "source": [
        "#Use make_blobs to generate values with a standard normal dis. \n",
        "#Predicitions appear consistant but having used different sample periods the predicitions do not chanage. likely biased. \n",
        "\n",
        "#new instances where we do not know the answer\n",
        "from sklearn.datasets import make_blobs\n",
        "Xnew, _ = make_blobs(n_samples=5, centers=5, n_features=6, random_state=1)\n",
        "# make a prediction\n",
        "ynew = svm_model.predict(Xnew)\n",
        "# show the inputs and predicted outputs\n",
        "for i in range(len(Xnew)):\n",
        "\tprint(\"Predicted=%s\" % (ynew[i]))"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted=1\n",
            "Predicted=0\n",
            "Predicted=1\n",
            "Predicted=1\n",
            "Predicted=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSE9ib5kerTd",
        "outputId": "45f76fb0-6274-4f4d-dae1-617e0e04f9a1"
      },
      "source": [
        "#forecast with log model as above \n",
        "from sklearn.datasets import make_blobs\n",
        "Xnew, _ = make_blobs(n_samples=5, centers=5, n_features=6, random_state=1)\n",
        "x_new = np.concatenate((x_test, Xnew))\n",
        "ynew = logmodel.predict(x_new)\n",
        "predictions = ynew[-5:]\n",
        "for i in range(len(predictions)):\n",
        "    print(\"Prediction = \", predictions[i])"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction =  1\n",
            "Prediction =  0\n",
            "Prediction =  1\n",
            "Prediction =  1\n",
            "Prediction =  0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exs1XJ5VbXex",
        "outputId": "9ad6d9d8-38c8-4a70-db4b-bcb809e112e8"
      },
      "source": [
        "#Forecasting with log model and random.choice \n",
        "#Use random choice to select 5 random rows of observations and add this onto the end of the time series test data to forecast 5 additional predicitions. \n",
        "#Results are very inconsistent \n",
        "xnew = np.random.choice(len(x_train), 5, replace=False)\n",
        "X_new = x_train[xnew]\n",
        "x_new = np.concatenate((x_test, X_new))\n",
        "ynew = logmodel.predict(x_new)\n",
        "predictions = ynew[-5:]\n",
        "for i in range(len(predictions)):\n",
        "    print(\"Prediction = \", predictions[i])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction =  1\n",
            "Prediction =  0\n",
            "Prediction =  1\n",
            "Prediction =  1\n",
            "Prediction =  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzPJV0kxdDCT",
        "outputId": "963e26cf-83af-4fb3-c9d1-d5bff9591cb5"
      },
      "source": [
        "#forecast using randit function to generate a random integer and effectively fit a random slice of the test data with our model plus number of observations.   \n",
        "#stable but bias results for this data set.\n",
        "from numpy import random\n",
        "\n",
        "random_number = random.randint(500)\n",
        "n_obs = 5\n",
        "random_set = x_test[random_number:random_number+n_obs]\n",
        "x_new = np.concatenate((x_test, random_set))\n",
        "\n",
        "ynew = logmodel.predict(x_new)\n",
        "predictions = ynew[-5:]\n",
        "for i in range(len(predictions)):\n",
        "    print(\"Prediction = \", predictions[i])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction =  1\n",
            "Prediction =  1\n",
            "Prediction =  1\n",
            "Prediction =  0\n",
            "Prediction =  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKEon5OCKwVk"
      },
      "source": [
        "# I will now form a DNN for predicting positive/negative returns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDKSdtH-pkAO"
      },
      "source": [
        "#reload data  \n",
        "df = pd.read_csv('/content/AAPL (3).csv') #load\n",
        "df['log return'] = np.log(df.Close) - np.log(df.Close.shift(1)) #calculate log returns and add col\n",
        "df['log return'] = [0 if i <=0 else 1 for i in df['log return']] #changes log returns to 0 or 1 for target\n",
        "df = df.drop([\"Date\"], axis=1) #drop date col\n",
        "\n",
        "df[columns_for_ft_scaling] = standardScaler.fit_transform(df[columns_for_ft_scaling]) #standard scale (I opted for this as model performed very badly with normalised data)\n",
        "\n",
        "x = np.array(df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]) #set x\n",
        "y = np.array(df['log return']) #set y \n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrdEK5hIwP41"
      },
      "source": [
        "## split into training and test data \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42, shuffle=True)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5Sr17UFXw4k",
        "outputId": "047bcaab-d3cc-49fa-a5f9-e7736f26aeb1"
      },
      "source": [
        "#build model by using keras sequential \n",
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(1024, input_dim=6, activation='relu')) #add first dense layer with 1024 nodes, relu activation function \n",
        "model.add(layers.Dense(512, activation='relu')) #add second dense layer with 512 nodes, relu activation function \n",
        "model.add(layers.Dense(256, activation='relu')) #third with 256 nodes, relu\n",
        "model.add(layers.Dense(128, activation='relu')) #fourth, relu \n",
        "model.add(layers.Dense(1, activation='sigmoid')) #output layer, sigmoid \n",
        "model.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 1024)              7168      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 696,321\n",
            "Trainable params: 696,321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceHgs-jAsOx0"
      },
      "source": [
        "#compile model, set loss function, optiser and metric\n",
        "from keras import optimizers\n",
        "optimizer = optimizers.Adam(lr=1e-05)\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer=optimizer,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ0_b5HOO_m9"
      },
      "source": [
        "#make tensor flow run eagerly instead of graph function \n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikz548TyZYDy",
        "outputId": "609d6789-f336-481b-b548-546dfca981e3"
      },
      "source": [
        "#fit model to training data \n",
        "model.fit(x_train, y_train, epochs=500, batch_size=128, validation_data=(x_test,y_test))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6831 - acc: 0.5548 - val_loss: 0.6830 - val_acc: 0.5620\n",
            "Epoch 2/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6830 - acc: 0.5602 - val_loss: 0.6829 - val_acc: 0.5601\n",
            "Epoch 3/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6828 - acc: 0.5539 - val_loss: 0.6826 - val_acc: 0.5638\n",
            "Epoch 4/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6827 - acc: 0.5590 - val_loss: 0.6825 - val_acc: 0.5638\n",
            "Epoch 5/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6826 - acc: 0.5557 - val_loss: 0.6824 - val_acc: 0.5666\n",
            "Epoch 6/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6824 - acc: 0.5553 - val_loss: 0.6819 - val_acc: 0.5694\n",
            "Epoch 7/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6823 - acc: 0.5588 - val_loss: 0.6821 - val_acc: 0.5722\n",
            "Epoch 8/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6826 - acc: 0.5623 - val_loss: 0.6823 - val_acc: 0.5620\n",
            "Epoch 9/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6822 - acc: 0.5520 - val_loss: 0.6818 - val_acc: 0.5694\n",
            "Epoch 10/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6819 - acc: 0.5574 - val_loss: 0.6816 - val_acc: 0.5676\n",
            "Epoch 11/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6818 - acc: 0.5623 - val_loss: 0.6818 - val_acc: 0.5657\n",
            "Epoch 12/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6816 - acc: 0.5590 - val_loss: 0.6813 - val_acc: 0.5666\n",
            "Epoch 13/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6815 - acc: 0.5623 - val_loss: 0.6817 - val_acc: 0.5629\n",
            "Epoch 14/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6813 - acc: 0.5562 - val_loss: 0.6812 - val_acc: 0.5732\n",
            "Epoch 15/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6818 - acc: 0.5606 - val_loss: 0.6810 - val_acc: 0.5834\n",
            "Epoch 16/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6811 - acc: 0.5704 - val_loss: 0.6818 - val_acc: 0.5648\n",
            "Epoch 17/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6815 - acc: 0.5618 - val_loss: 0.6813 - val_acc: 0.5610\n",
            "Epoch 18/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6808 - acc: 0.5611 - val_loss: 0.6809 - val_acc: 0.5694\n",
            "Epoch 19/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6805 - acc: 0.5667 - val_loss: 0.6811 - val_acc: 0.5685\n",
            "Epoch 20/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6806 - acc: 0.5641 - val_loss: 0.6813 - val_acc: 0.5601\n",
            "Epoch 21/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6805 - acc: 0.5630 - val_loss: 0.6803 - val_acc: 0.5685\n",
            "Epoch 22/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6807 - acc: 0.5602 - val_loss: 0.6798 - val_acc: 0.5750\n",
            "Epoch 23/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6800 - acc: 0.5660 - val_loss: 0.6803 - val_acc: 0.5713\n",
            "Epoch 24/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6798 - acc: 0.5630 - val_loss: 0.6798 - val_acc: 0.5685\n",
            "Epoch 25/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6796 - acc: 0.5625 - val_loss: 0.6801 - val_acc: 0.5676\n",
            "Epoch 26/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6793 - acc: 0.5625 - val_loss: 0.6796 - val_acc: 0.5713\n",
            "Epoch 27/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6791 - acc: 0.5639 - val_loss: 0.6792 - val_acc: 0.5778\n",
            "Epoch 28/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6793 - acc: 0.5676 - val_loss: 0.6794 - val_acc: 0.5676\n",
            "Epoch 29/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6791 - acc: 0.5655 - val_loss: 0.6789 - val_acc: 0.5788\n",
            "Epoch 30/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6788 - acc: 0.5849 - val_loss: 0.6809 - val_acc: 0.5629\n",
            "Epoch 31/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6789 - acc: 0.5658 - val_loss: 0.6783 - val_acc: 0.5881\n",
            "Epoch 32/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6786 - acc: 0.5702 - val_loss: 0.6779 - val_acc: 0.5806\n",
            "Epoch 33/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6783 - acc: 0.5728 - val_loss: 0.6781 - val_acc: 0.5871\n",
            "Epoch 34/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6779 - acc: 0.5744 - val_loss: 0.6784 - val_acc: 0.5732\n",
            "Epoch 35/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6778 - acc: 0.5690 - val_loss: 0.6777 - val_acc: 0.5918\n",
            "Epoch 36/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6779 - acc: 0.5704 - val_loss: 0.6780 - val_acc: 0.5806\n",
            "Epoch 37/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6779 - acc: 0.5791 - val_loss: 0.6778 - val_acc: 0.5732\n",
            "Epoch 38/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6772 - acc: 0.5707 - val_loss: 0.6774 - val_acc: 0.5806\n",
            "Epoch 39/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6771 - acc: 0.5667 - val_loss: 0.6771 - val_acc: 0.5825\n",
            "Epoch 40/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6767 - acc: 0.5779 - val_loss: 0.6770 - val_acc: 0.5843\n",
            "Epoch 41/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6765 - acc: 0.5737 - val_loss: 0.6764 - val_acc: 0.5853\n",
            "Epoch 42/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6764 - acc: 0.5735 - val_loss: 0.6766 - val_acc: 0.5862\n",
            "Epoch 43/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6763 - acc: 0.5798 - val_loss: 0.6771 - val_acc: 0.5834\n",
            "Epoch 44/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6757 - acc: 0.5728 - val_loss: 0.6774 - val_acc: 0.5732\n",
            "Epoch 45/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6761 - acc: 0.5739 - val_loss: 0.6758 - val_acc: 0.5955\n",
            "Epoch 46/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6759 - acc: 0.5800 - val_loss: 0.6759 - val_acc: 0.5881\n",
            "Epoch 47/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6753 - acc: 0.5865 - val_loss: 0.6757 - val_acc: 0.5843\n",
            "Epoch 48/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6750 - acc: 0.5807 - val_loss: 0.6758 - val_acc: 0.5862\n",
            "Epoch 49/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6747 - acc: 0.5842 - val_loss: 0.6765 - val_acc: 0.5750\n",
            "Epoch 50/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6742 - acc: 0.5781 - val_loss: 0.6747 - val_acc: 0.5853\n",
            "Epoch 51/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6747 - acc: 0.5842 - val_loss: 0.6773 - val_acc: 0.5685\n",
            "Epoch 52/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6738 - acc: 0.5735 - val_loss: 0.6741 - val_acc: 0.5927\n",
            "Epoch 53/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6738 - acc: 0.5868 - val_loss: 0.6743 - val_acc: 0.5983\n",
            "Epoch 54/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6736 - acc: 0.5889 - val_loss: 0.6761 - val_acc: 0.5788\n",
            "Epoch 55/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6734 - acc: 0.5854 - val_loss: 0.6733 - val_acc: 0.5871\n",
            "Epoch 56/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6731 - acc: 0.5791 - val_loss: 0.6737 - val_acc: 0.6123\n",
            "Epoch 57/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6741 - acc: 0.5837 - val_loss: 0.6733 - val_acc: 0.6207\n",
            "Epoch 58/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6727 - acc: 0.5870 - val_loss: 0.6742 - val_acc: 0.5862\n",
            "Epoch 59/500\n",
            "34/34 [==============================] - 1s 15ms/step - loss: 0.6723 - acc: 0.5938 - val_loss: 0.6731 - val_acc: 0.5890\n",
            "Epoch 60/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6720 - acc: 0.5872 - val_loss: 0.6730 - val_acc: 0.5881\n",
            "Epoch 61/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6718 - acc: 0.5865 - val_loss: 0.6728 - val_acc: 0.5909\n",
            "Epoch 62/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6722 - acc: 0.5931 - val_loss: 0.6724 - val_acc: 0.5899\n",
            "Epoch 63/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6725 - acc: 0.5912 - val_loss: 0.6731 - val_acc: 0.5862\n",
            "Epoch 64/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6716 - acc: 0.5877 - val_loss: 0.6720 - val_acc: 0.5890\n",
            "Epoch 65/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6706 - acc: 0.5926 - val_loss: 0.6717 - val_acc: 0.6067\n",
            "Epoch 66/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6710 - acc: 0.5975 - val_loss: 0.6768 - val_acc: 0.5694\n",
            "Epoch 67/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6702 - acc: 0.5898 - val_loss: 0.6708 - val_acc: 0.5918\n",
            "Epoch 68/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6700 - acc: 0.5951 - val_loss: 0.6724 - val_acc: 0.5899\n",
            "Epoch 69/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6694 - acc: 0.5884 - val_loss: 0.6707 - val_acc: 0.5890\n",
            "Epoch 70/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6691 - acc: 0.5961 - val_loss: 0.6704 - val_acc: 0.6058\n",
            "Epoch 71/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6686 - acc: 0.5972 - val_loss: 0.6726 - val_acc: 0.5843\n",
            "Epoch 72/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6688 - acc: 0.5942 - val_loss: 0.6717 - val_acc: 0.5918\n",
            "Epoch 73/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6683 - acc: 0.5954 - val_loss: 0.6695 - val_acc: 0.5993\n",
            "Epoch 74/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6687 - acc: 0.5935 - val_loss: 0.6700 - val_acc: 0.5937\n",
            "Epoch 75/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6680 - acc: 0.5963 - val_loss: 0.6696 - val_acc: 0.5918\n",
            "Epoch 76/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6680 - acc: 0.5979 - val_loss: 0.6689 - val_acc: 0.6095\n",
            "Epoch 77/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6679 - acc: 0.5942 - val_loss: 0.6712 - val_acc: 0.5899\n",
            "Epoch 78/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6677 - acc: 0.5986 - val_loss: 0.6736 - val_acc: 0.5834\n",
            "Epoch 79/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6672 - acc: 0.5970 - val_loss: 0.6693 - val_acc: 0.5927\n",
            "Epoch 80/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6669 - acc: 0.6024 - val_loss: 0.6726 - val_acc: 0.5806\n",
            "Epoch 81/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6668 - acc: 0.5949 - val_loss: 0.6675 - val_acc: 0.6179\n",
            "Epoch 82/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6660 - acc: 0.6010 - val_loss: 0.6676 - val_acc: 0.6244\n",
            "Epoch 83/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6663 - acc: 0.5989 - val_loss: 0.6675 - val_acc: 0.6058\n",
            "Epoch 84/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6652 - acc: 0.6091 - val_loss: 0.6673 - val_acc: 0.6021\n",
            "Epoch 85/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6656 - acc: 0.5942 - val_loss: 0.6663 - val_acc: 0.6076\n",
            "Epoch 86/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6647 - acc: 0.6084 - val_loss: 0.6663 - val_acc: 0.6095\n",
            "Epoch 87/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6645 - acc: 0.6021 - val_loss: 0.6664 - val_acc: 0.6076\n",
            "Epoch 88/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6648 - acc: 0.6056 - val_loss: 0.6664 - val_acc: 0.5993\n",
            "Epoch 89/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6640 - acc: 0.6063 - val_loss: 0.6666 - val_acc: 0.6002\n",
            "Epoch 90/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6633 - acc: 0.6089 - val_loss: 0.6648 - val_acc: 0.6216\n",
            "Epoch 91/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6635 - acc: 0.6063 - val_loss: 0.6667 - val_acc: 0.5881\n",
            "Epoch 92/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6629 - acc: 0.6026 - val_loss: 0.6644 - val_acc: 0.6253\n",
            "Epoch 93/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6624 - acc: 0.6073 - val_loss: 0.6641 - val_acc: 0.6226\n",
            "Epoch 94/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6623 - acc: 0.6042 - val_loss: 0.6643 - val_acc: 0.6086\n",
            "Epoch 95/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6622 - acc: 0.6052 - val_loss: 0.6643 - val_acc: 0.6114\n",
            "Epoch 96/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6623 - acc: 0.6063 - val_loss: 0.6634 - val_acc: 0.6216\n",
            "Epoch 97/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6623 - acc: 0.6021 - val_loss: 0.6630 - val_acc: 0.6207\n",
            "Epoch 98/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6616 - acc: 0.6080 - val_loss: 0.6631 - val_acc: 0.6207\n",
            "Epoch 99/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6607 - acc: 0.6112 - val_loss: 0.6635 - val_acc: 0.6002\n",
            "Epoch 100/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6599 - acc: 0.6105 - val_loss: 0.6620 - val_acc: 0.6309\n",
            "Epoch 101/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6602 - acc: 0.6126 - val_loss: 0.6635 - val_acc: 0.6076\n",
            "Epoch 102/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6599 - acc: 0.6070 - val_loss: 0.6610 - val_acc: 0.6300\n",
            "Epoch 103/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6605 - acc: 0.6112 - val_loss: 0.6620 - val_acc: 0.6235\n",
            "Epoch 104/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6589 - acc: 0.6054 - val_loss: 0.6612 - val_acc: 0.6104\n",
            "Epoch 105/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6589 - acc: 0.6105 - val_loss: 0.6610 - val_acc: 0.6188\n",
            "Epoch 106/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6587 - acc: 0.6087 - val_loss: 0.6620 - val_acc: 0.6179\n",
            "Epoch 107/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6584 - acc: 0.6168 - val_loss: 0.6608 - val_acc: 0.6216\n",
            "Epoch 108/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6582 - acc: 0.6070 - val_loss: 0.6619 - val_acc: 0.6030\n",
            "Epoch 109/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6577 - acc: 0.6110 - val_loss: 0.6628 - val_acc: 0.6021\n",
            "Epoch 110/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6574 - acc: 0.6157 - val_loss: 0.6598 - val_acc: 0.6309\n",
            "Epoch 111/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6564 - acc: 0.6161 - val_loss: 0.6626 - val_acc: 0.6048\n",
            "Epoch 112/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6571 - acc: 0.6119 - val_loss: 0.6597 - val_acc: 0.6067\n",
            "Epoch 113/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6559 - acc: 0.6143 - val_loss: 0.6589 - val_acc: 0.6235\n",
            "Epoch 114/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6565 - acc: 0.6185 - val_loss: 0.6593 - val_acc: 0.6160\n",
            "Epoch 115/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6555 - acc: 0.6213 - val_loss: 0.6588 - val_acc: 0.6216\n",
            "Epoch 116/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6551 - acc: 0.6194 - val_loss: 0.6581 - val_acc: 0.6272\n",
            "Epoch 117/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6549 - acc: 0.6166 - val_loss: 0.6586 - val_acc: 0.6216\n",
            "Epoch 118/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6545 - acc: 0.6192 - val_loss: 0.6568 - val_acc: 0.6291\n",
            "Epoch 119/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6546 - acc: 0.6124 - val_loss: 0.6583 - val_acc: 0.6272\n",
            "Epoch 120/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6549 - acc: 0.6140 - val_loss: 0.6559 - val_acc: 0.6337\n",
            "Epoch 121/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6537 - acc: 0.6189 - val_loss: 0.6572 - val_acc: 0.6244\n",
            "Epoch 122/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6541 - acc: 0.6138 - val_loss: 0.6659 - val_acc: 0.5927\n",
            "Epoch 123/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6536 - acc: 0.6166 - val_loss: 0.6555 - val_acc: 0.6319\n",
            "Epoch 124/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6532 - acc: 0.6185 - val_loss: 0.6560 - val_acc: 0.6356\n",
            "Epoch 125/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6534 - acc: 0.6194 - val_loss: 0.6548 - val_acc: 0.6281\n",
            "Epoch 126/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6522 - acc: 0.6185 - val_loss: 0.6548 - val_acc: 0.6272\n",
            "Epoch 127/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6519 - acc: 0.6192 - val_loss: 0.6543 - val_acc: 0.6309\n",
            "Epoch 128/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6511 - acc: 0.6194 - val_loss: 0.6541 - val_acc: 0.6253\n",
            "Epoch 129/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6505 - acc: 0.6290 - val_loss: 0.6545 - val_acc: 0.6263\n",
            "Epoch 130/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6500 - acc: 0.6318 - val_loss: 0.6566 - val_acc: 0.6095\n",
            "Epoch 131/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6500 - acc: 0.6241 - val_loss: 0.6540 - val_acc: 0.6253\n",
            "Epoch 132/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6502 - acc: 0.6278 - val_loss: 0.6534 - val_acc: 0.6300\n",
            "Epoch 133/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6493 - acc: 0.6264 - val_loss: 0.6552 - val_acc: 0.6188\n",
            "Epoch 134/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6490 - acc: 0.6276 - val_loss: 0.6556 - val_acc: 0.6253\n",
            "Epoch 135/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6493 - acc: 0.6311 - val_loss: 0.6564 - val_acc: 0.6207\n",
            "Epoch 136/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6488 - acc: 0.6248 - val_loss: 0.6538 - val_acc: 0.6263\n",
            "Epoch 137/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6483 - acc: 0.6241 - val_loss: 0.6521 - val_acc: 0.6300\n",
            "Epoch 138/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6472 - acc: 0.6306 - val_loss: 0.6511 - val_acc: 0.6403\n",
            "Epoch 139/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6482 - acc: 0.6255 - val_loss: 0.6505 - val_acc: 0.6375\n",
            "Epoch 140/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6482 - acc: 0.6185 - val_loss: 0.6503 - val_acc: 0.6263\n",
            "Epoch 141/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6466 - acc: 0.6245 - val_loss: 0.6498 - val_acc: 0.6365\n",
            "Epoch 142/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6464 - acc: 0.6250 - val_loss: 0.6511 - val_acc: 0.6328\n",
            "Epoch 143/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6462 - acc: 0.6332 - val_loss: 0.6509 - val_acc: 0.6309\n",
            "Epoch 144/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6458 - acc: 0.6329 - val_loss: 0.6493 - val_acc: 0.6393\n",
            "Epoch 145/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6459 - acc: 0.6255 - val_loss: 0.6489 - val_acc: 0.6291\n",
            "Epoch 146/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6455 - acc: 0.6292 - val_loss: 0.6492 - val_acc: 0.6393\n",
            "Epoch 147/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6450 - acc: 0.6245 - val_loss: 0.6478 - val_acc: 0.6431\n",
            "Epoch 148/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6461 - acc: 0.6248 - val_loss: 0.6497 - val_acc: 0.6319\n",
            "Epoch 149/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6440 - acc: 0.6299 - val_loss: 0.6474 - val_acc: 0.6421\n",
            "Epoch 150/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6433 - acc: 0.6278 - val_loss: 0.6481 - val_acc: 0.6272\n",
            "Epoch 151/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6439 - acc: 0.6304 - val_loss: 0.6509 - val_acc: 0.6300\n",
            "Epoch 152/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6444 - acc: 0.6250 - val_loss: 0.6461 - val_acc: 0.6393\n",
            "Epoch 153/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.6433 - acc: 0.6311 - val_loss: 0.6462 - val_acc: 0.6403\n",
            "Epoch 154/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6429 - acc: 0.6285 - val_loss: 0.6478 - val_acc: 0.6328\n",
            "Epoch 155/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6437 - acc: 0.6227 - val_loss: 0.6466 - val_acc: 0.6337\n",
            "Epoch 156/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6423 - acc: 0.6339 - val_loss: 0.6466 - val_acc: 0.6347\n",
            "Epoch 157/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6415 - acc: 0.6360 - val_loss: 0.6483 - val_acc: 0.6337\n",
            "Epoch 158/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6423 - acc: 0.6224 - val_loss: 0.6461 - val_acc: 0.6319\n",
            "Epoch 159/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6411 - acc: 0.6357 - val_loss: 0.6471 - val_acc: 0.6356\n",
            "Epoch 160/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6408 - acc: 0.6334 - val_loss: 0.6440 - val_acc: 0.6468\n",
            "Epoch 161/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6406 - acc: 0.6287 - val_loss: 0.6453 - val_acc: 0.6431\n",
            "Epoch 162/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6405 - acc: 0.6362 - val_loss: 0.6440 - val_acc: 0.6431\n",
            "Epoch 163/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6403 - acc: 0.6299 - val_loss: 0.6475 - val_acc: 0.6319\n",
            "Epoch 164/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6445 - acc: 0.6185 - val_loss: 0.6433 - val_acc: 0.6421\n",
            "Epoch 165/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6399 - acc: 0.6313 - val_loss: 0.6440 - val_acc: 0.6459\n",
            "Epoch 166/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6380 - acc: 0.6320 - val_loss: 0.6460 - val_acc: 0.6263\n",
            "Epoch 167/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6400 - acc: 0.6224 - val_loss: 0.6485 - val_acc: 0.6375\n",
            "Epoch 168/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6392 - acc: 0.6297 - val_loss: 0.6427 - val_acc: 0.6449\n",
            "Epoch 169/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6380 - acc: 0.6343 - val_loss: 0.6430 - val_acc: 0.6393\n",
            "Epoch 170/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6381 - acc: 0.6287 - val_loss: 0.6422 - val_acc: 0.6328\n",
            "Epoch 171/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6369 - acc: 0.6367 - val_loss: 0.6461 - val_acc: 0.6412\n",
            "Epoch 172/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6369 - acc: 0.6308 - val_loss: 0.6436 - val_acc: 0.6337\n",
            "Epoch 173/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6369 - acc: 0.6339 - val_loss: 0.6440 - val_acc: 0.6403\n",
            "Epoch 174/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6364 - acc: 0.6353 - val_loss: 0.6413 - val_acc: 0.6468\n",
            "Epoch 175/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6368 - acc: 0.6346 - val_loss: 0.6402 - val_acc: 0.6477\n",
            "Epoch 176/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6357 - acc: 0.6350 - val_loss: 0.6436 - val_acc: 0.6356\n",
            "Epoch 177/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6390 - acc: 0.6294 - val_loss: 0.6409 - val_acc: 0.6412\n",
            "Epoch 178/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6357 - acc: 0.6346 - val_loss: 0.6393 - val_acc: 0.6431\n",
            "Epoch 179/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6345 - acc: 0.6409 - val_loss: 0.6418 - val_acc: 0.6421\n",
            "Epoch 180/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6342 - acc: 0.6343 - val_loss: 0.6416 - val_acc: 0.6347\n",
            "Epoch 181/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6343 - acc: 0.6458 - val_loss: 0.6398 - val_acc: 0.6570\n",
            "Epoch 182/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6349 - acc: 0.6374 - val_loss: 0.6393 - val_acc: 0.6459\n",
            "Epoch 183/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6331 - acc: 0.6402 - val_loss: 0.6448 - val_acc: 0.6328\n",
            "Epoch 184/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6331 - acc: 0.6397 - val_loss: 0.6393 - val_acc: 0.6384\n",
            "Epoch 185/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6332 - acc: 0.6308 - val_loss: 0.6388 - val_acc: 0.6337\n",
            "Epoch 186/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6336 - acc: 0.6369 - val_loss: 0.6385 - val_acc: 0.6496\n",
            "Epoch 187/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6327 - acc: 0.6446 - val_loss: 0.6373 - val_acc: 0.6524\n",
            "Epoch 188/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6318 - acc: 0.6446 - val_loss: 0.6369 - val_acc: 0.6468\n",
            "Epoch 189/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6325 - acc: 0.6362 - val_loss: 0.6358 - val_acc: 0.6403\n",
            "Epoch 190/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6315 - acc: 0.6425 - val_loss: 0.6379 - val_acc: 0.6533\n",
            "Epoch 191/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6338 - acc: 0.6409 - val_loss: 0.6382 - val_acc: 0.6468\n",
            "Epoch 192/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6316 - acc: 0.6369 - val_loss: 0.6360 - val_acc: 0.6505\n",
            "Epoch 193/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6325 - acc: 0.6409 - val_loss: 0.6368 - val_acc: 0.6505\n",
            "Epoch 194/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6306 - acc: 0.6427 - val_loss: 0.6347 - val_acc: 0.6514\n",
            "Epoch 195/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6327 - acc: 0.6420 - val_loss: 0.6442 - val_acc: 0.6375\n",
            "Epoch 196/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6297 - acc: 0.6409 - val_loss: 0.6348 - val_acc: 0.6514\n",
            "Epoch 197/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6299 - acc: 0.6425 - val_loss: 0.6346 - val_acc: 0.6486\n",
            "Epoch 198/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6293 - acc: 0.6448 - val_loss: 0.6350 - val_acc: 0.6477\n",
            "Epoch 199/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6287 - acc: 0.6404 - val_loss: 0.6340 - val_acc: 0.6514\n",
            "Epoch 200/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6291 - acc: 0.6439 - val_loss: 0.6357 - val_acc: 0.6533\n",
            "Epoch 201/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6305 - acc: 0.6448 - val_loss: 0.6356 - val_acc: 0.6375\n",
            "Epoch 202/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6287 - acc: 0.6490 - val_loss: 0.6327 - val_acc: 0.6542\n",
            "Epoch 203/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6281 - acc: 0.6474 - val_loss: 0.6336 - val_acc: 0.6561\n",
            "Epoch 204/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6281 - acc: 0.6483 - val_loss: 0.6332 - val_acc: 0.6533\n",
            "Epoch 205/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6273 - acc: 0.6472 - val_loss: 0.6338 - val_acc: 0.6542\n",
            "Epoch 206/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6277 - acc: 0.6453 - val_loss: 0.6339 - val_acc: 0.6459\n",
            "Epoch 207/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6269 - acc: 0.6427 - val_loss: 0.6315 - val_acc: 0.6403\n",
            "Epoch 208/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6277 - acc: 0.6444 - val_loss: 0.6335 - val_acc: 0.6412\n",
            "Epoch 209/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6267 - acc: 0.6383 - val_loss: 0.6312 - val_acc: 0.6505\n",
            "Epoch 210/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6269 - acc: 0.6411 - val_loss: 0.6381 - val_acc: 0.6375\n",
            "Epoch 211/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6259 - acc: 0.6467 - val_loss: 0.6318 - val_acc: 0.6608\n",
            "Epoch 212/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6259 - acc: 0.6465 - val_loss: 0.6301 - val_acc: 0.6570\n",
            "Epoch 213/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6254 - acc: 0.6544 - val_loss: 0.6300 - val_acc: 0.6589\n",
            "Epoch 214/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6255 - acc: 0.6514 - val_loss: 0.6332 - val_acc: 0.6589\n",
            "Epoch 215/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6252 - acc: 0.6455 - val_loss: 0.6307 - val_acc: 0.6375\n",
            "Epoch 216/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6248 - acc: 0.6483 - val_loss: 0.6307 - val_acc: 0.6524\n",
            "Epoch 217/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6239 - acc: 0.6523 - val_loss: 0.6317 - val_acc: 0.6608\n",
            "Epoch 218/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6237 - acc: 0.6446 - val_loss: 0.6289 - val_acc: 0.6580\n",
            "Epoch 219/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6257 - acc: 0.6413 - val_loss: 0.6353 - val_acc: 0.6319\n",
            "Epoch 220/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6245 - acc: 0.6402 - val_loss: 0.6285 - val_acc: 0.6608\n",
            "Epoch 221/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6236 - acc: 0.6427 - val_loss: 0.6302 - val_acc: 0.6468\n",
            "Epoch 222/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6251 - acc: 0.6472 - val_loss: 0.6275 - val_acc: 0.6570\n",
            "Epoch 223/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6244 - acc: 0.6444 - val_loss: 0.6296 - val_acc: 0.6403\n",
            "Epoch 224/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6225 - acc: 0.6469 - val_loss: 0.6292 - val_acc: 0.6505\n",
            "Epoch 225/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6223 - acc: 0.6448 - val_loss: 0.6293 - val_acc: 0.6533\n",
            "Epoch 226/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6219 - acc: 0.6483 - val_loss: 0.6293 - val_acc: 0.6542\n",
            "Epoch 227/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6205 - acc: 0.6455 - val_loss: 0.6298 - val_acc: 0.6347\n",
            "Epoch 228/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6241 - acc: 0.6413 - val_loss: 0.6339 - val_acc: 0.6486\n",
            "Epoch 229/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6215 - acc: 0.6535 - val_loss: 0.6283 - val_acc: 0.6514\n",
            "Epoch 230/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6207 - acc: 0.6535 - val_loss: 0.6307 - val_acc: 0.6645\n",
            "Epoch 231/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6201 - acc: 0.6553 - val_loss: 0.6283 - val_acc: 0.6673\n",
            "Epoch 232/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6204 - acc: 0.6516 - val_loss: 0.6256 - val_acc: 0.6468\n",
            "Epoch 233/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6196 - acc: 0.6495 - val_loss: 0.6284 - val_acc: 0.6496\n",
            "Epoch 234/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6204 - acc: 0.6553 - val_loss: 0.6268 - val_acc: 0.6505\n",
            "Epoch 235/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6186 - acc: 0.6562 - val_loss: 0.6242 - val_acc: 0.6626\n",
            "Epoch 236/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6190 - acc: 0.6553 - val_loss: 0.6256 - val_acc: 0.6542\n",
            "Epoch 237/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6191 - acc: 0.6562 - val_loss: 0.6241 - val_acc: 0.6542\n",
            "Epoch 238/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6185 - acc: 0.6569 - val_loss: 0.6269 - val_acc: 0.6701\n",
            "Epoch 239/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6187 - acc: 0.6567 - val_loss: 0.6253 - val_acc: 0.6636\n",
            "Epoch 240/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6182 - acc: 0.6574 - val_loss: 0.6253 - val_acc: 0.6505\n",
            "Epoch 241/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6177 - acc: 0.6569 - val_loss: 0.6223 - val_acc: 0.6636\n",
            "Epoch 242/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6201 - acc: 0.6476 - val_loss: 0.6245 - val_acc: 0.6673\n",
            "Epoch 243/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6181 - acc: 0.6532 - val_loss: 0.6227 - val_acc: 0.6598\n",
            "Epoch 244/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6174 - acc: 0.6581 - val_loss: 0.6221 - val_acc: 0.6477\n",
            "Epoch 245/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6164 - acc: 0.6642 - val_loss: 0.6340 - val_acc: 0.6580\n",
            "Epoch 246/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6174 - acc: 0.6569 - val_loss: 0.6243 - val_acc: 0.6580\n",
            "Epoch 247/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6167 - acc: 0.6560 - val_loss: 0.6234 - val_acc: 0.6608\n",
            "Epoch 248/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6165 - acc: 0.6595 - val_loss: 0.6212 - val_acc: 0.6468\n",
            "Epoch 249/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6163 - acc: 0.6621 - val_loss: 0.6291 - val_acc: 0.6412\n",
            "Epoch 250/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6159 - acc: 0.6544 - val_loss: 0.6263 - val_acc: 0.6580\n",
            "Epoch 251/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6150 - acc: 0.6639 - val_loss: 0.6231 - val_acc: 0.6496\n",
            "Epoch 252/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6163 - acc: 0.6574 - val_loss: 0.6248 - val_acc: 0.6486\n",
            "Epoch 253/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6154 - acc: 0.6614 - val_loss: 0.6210 - val_acc: 0.6692\n",
            "Epoch 254/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6144 - acc: 0.6553 - val_loss: 0.6215 - val_acc: 0.6757\n",
            "Epoch 255/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6132 - acc: 0.6646 - val_loss: 0.6257 - val_acc: 0.6459\n",
            "Epoch 256/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6152 - acc: 0.6583 - val_loss: 0.6224 - val_acc: 0.6645\n",
            "Epoch 257/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6139 - acc: 0.6686 - val_loss: 0.6189 - val_acc: 0.6654\n",
            "Epoch 258/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6137 - acc: 0.6639 - val_loss: 0.6248 - val_acc: 0.6486\n",
            "Epoch 259/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6134 - acc: 0.6623 - val_loss: 0.6231 - val_acc: 0.6617\n",
            "Epoch 260/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6126 - acc: 0.6628 - val_loss: 0.6185 - val_acc: 0.6626\n",
            "Epoch 261/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6126 - acc: 0.6542 - val_loss: 0.6182 - val_acc: 0.6673\n",
            "Epoch 262/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6130 - acc: 0.6670 - val_loss: 0.6219 - val_acc: 0.6664\n",
            "Epoch 263/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6122 - acc: 0.6656 - val_loss: 0.6237 - val_acc: 0.6589\n",
            "Epoch 264/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6119 - acc: 0.6646 - val_loss: 0.6158 - val_acc: 0.6710\n",
            "Epoch 265/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6110 - acc: 0.6719 - val_loss: 0.6187 - val_acc: 0.6589\n",
            "Epoch 266/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6126 - acc: 0.6618 - val_loss: 0.6169 - val_acc: 0.6673\n",
            "Epoch 267/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6149 - acc: 0.6565 - val_loss: 0.6264 - val_acc: 0.6496\n",
            "Epoch 268/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6107 - acc: 0.6614 - val_loss: 0.6239 - val_acc: 0.6626\n",
            "Epoch 269/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6100 - acc: 0.6679 - val_loss: 0.6194 - val_acc: 0.6757\n",
            "Epoch 270/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6126 - acc: 0.6681 - val_loss: 0.6155 - val_acc: 0.6664\n",
            "Epoch 271/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6112 - acc: 0.6632 - val_loss: 0.6175 - val_acc: 0.6608\n",
            "Epoch 272/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6092 - acc: 0.6623 - val_loss: 0.6169 - val_acc: 0.6664\n",
            "Epoch 273/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6096 - acc: 0.6642 - val_loss: 0.6152 - val_acc: 0.6719\n",
            "Epoch 274/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6097 - acc: 0.6674 - val_loss: 0.6175 - val_acc: 0.6608\n",
            "Epoch 275/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6090 - acc: 0.6609 - val_loss: 0.6233 - val_acc: 0.6580\n",
            "Epoch 276/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6102 - acc: 0.6635 - val_loss: 0.6136 - val_acc: 0.6701\n",
            "Epoch 277/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6081 - acc: 0.6623 - val_loss: 0.6147 - val_acc: 0.6785\n",
            "Epoch 278/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6074 - acc: 0.6716 - val_loss: 0.6142 - val_acc: 0.6664\n",
            "Epoch 279/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6111 - acc: 0.6665 - val_loss: 0.6133 - val_acc: 0.6645\n",
            "Epoch 280/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6078 - acc: 0.6679 - val_loss: 0.6189 - val_acc: 0.6589\n",
            "Epoch 281/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6093 - acc: 0.6544 - val_loss: 0.6129 - val_acc: 0.6747\n",
            "Epoch 282/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6062 - acc: 0.6775 - val_loss: 0.6183 - val_acc: 0.6664\n",
            "Epoch 283/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6074 - acc: 0.6628 - val_loss: 0.6122 - val_acc: 0.6794\n",
            "Epoch 284/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6088 - acc: 0.6686 - val_loss: 0.6139 - val_acc: 0.6785\n",
            "Epoch 285/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6070 - acc: 0.6679 - val_loss: 0.6122 - val_acc: 0.6831\n",
            "Epoch 286/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6070 - acc: 0.6775 - val_loss: 0.6147 - val_acc: 0.6626\n",
            "Epoch 287/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6055 - acc: 0.6751 - val_loss: 0.6114 - val_acc: 0.6794\n",
            "Epoch 288/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6049 - acc: 0.6756 - val_loss: 0.6177 - val_acc: 0.6636\n",
            "Epoch 289/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6074 - acc: 0.6665 - val_loss: 0.6126 - val_acc: 0.6664\n",
            "Epoch 290/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6058 - acc: 0.6756 - val_loss: 0.6103 - val_acc: 0.6850\n",
            "Epoch 291/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6047 - acc: 0.6779 - val_loss: 0.6119 - val_acc: 0.6747\n",
            "Epoch 292/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6044 - acc: 0.6686 - val_loss: 0.6133 - val_acc: 0.6859\n",
            "Epoch 293/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6061 - acc: 0.6742 - val_loss: 0.6106 - val_acc: 0.6673\n",
            "Epoch 294/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6047 - acc: 0.6756 - val_loss: 0.6123 - val_acc: 0.6636\n",
            "Epoch 295/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6043 - acc: 0.6667 - val_loss: 0.6143 - val_acc: 0.6813\n",
            "Epoch 296/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6055 - acc: 0.6747 - val_loss: 0.6091 - val_acc: 0.6841\n",
            "Epoch 297/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6032 - acc: 0.6761 - val_loss: 0.6109 - val_acc: 0.6645\n",
            "Epoch 298/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6031 - acc: 0.6779 - val_loss: 0.6096 - val_acc: 0.6738\n",
            "Epoch 299/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6030 - acc: 0.6761 - val_loss: 0.6090 - val_acc: 0.6729\n",
            "Epoch 300/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6035 - acc: 0.6726 - val_loss: 0.6117 - val_acc: 0.6822\n",
            "Epoch 301/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6056 - acc: 0.6637 - val_loss: 0.6127 - val_acc: 0.6869\n",
            "Epoch 302/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6047 - acc: 0.6751 - val_loss: 0.6123 - val_acc: 0.6580\n",
            "Epoch 303/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.6040 - acc: 0.6807 - val_loss: 0.6102 - val_acc: 0.6775\n",
            "Epoch 304/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6019 - acc: 0.6838 - val_loss: 0.6148 - val_acc: 0.6570\n",
            "Epoch 305/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6018 - acc: 0.6719 - val_loss: 0.6085 - val_acc: 0.6841\n",
            "Epoch 306/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6011 - acc: 0.6765 - val_loss: 0.6106 - val_acc: 0.6822\n",
            "Epoch 307/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6021 - acc: 0.6749 - val_loss: 0.6153 - val_acc: 0.6580\n",
            "Epoch 308/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6011 - acc: 0.6723 - val_loss: 0.6125 - val_acc: 0.6729\n",
            "Epoch 309/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6019 - acc: 0.6728 - val_loss: 0.6137 - val_acc: 0.6654\n",
            "Epoch 310/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6012 - acc: 0.6763 - val_loss: 0.6072 - val_acc: 0.6822\n",
            "Epoch 311/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6015 - acc: 0.6779 - val_loss: 0.6086 - val_acc: 0.6869\n",
            "Epoch 312/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6009 - acc: 0.6740 - val_loss: 0.6067 - val_acc: 0.6915\n",
            "Epoch 313/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6001 - acc: 0.6798 - val_loss: 0.6064 - val_acc: 0.6841\n",
            "Epoch 314/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6013 - acc: 0.6828 - val_loss: 0.6104 - val_acc: 0.6897\n",
            "Epoch 315/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6004 - acc: 0.6751 - val_loss: 0.6074 - val_acc: 0.6710\n",
            "Epoch 316/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5994 - acc: 0.6772 - val_loss: 0.6158 - val_acc: 0.6524\n",
            "Epoch 317/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6002 - acc: 0.6733 - val_loss: 0.6099 - val_acc: 0.6682\n",
            "Epoch 318/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5999 - acc: 0.6765 - val_loss: 0.6208 - val_acc: 0.6505\n",
            "Epoch 319/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6013 - acc: 0.6681 - val_loss: 0.6056 - val_acc: 0.6626\n",
            "Epoch 320/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6003 - acc: 0.6674 - val_loss: 0.6061 - val_acc: 0.6710\n",
            "Epoch 321/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5998 - acc: 0.6740 - val_loss: 0.6048 - val_acc: 0.6925\n",
            "Epoch 322/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6010 - acc: 0.6695 - val_loss: 0.6072 - val_acc: 0.6766\n",
            "Epoch 323/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.6003 - acc: 0.6782 - val_loss: 0.6032 - val_acc: 0.6747\n",
            "Epoch 324/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5993 - acc: 0.6779 - val_loss: 0.6069 - val_acc: 0.6962\n",
            "Epoch 325/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5962 - acc: 0.6812 - val_loss: 0.6048 - val_acc: 0.6692\n",
            "Epoch 326/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5994 - acc: 0.6754 - val_loss: 0.6050 - val_acc: 0.6710\n",
            "Epoch 327/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5986 - acc: 0.6803 - val_loss: 0.6046 - val_acc: 0.6738\n",
            "Epoch 328/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5984 - acc: 0.6761 - val_loss: 0.6121 - val_acc: 0.6589\n",
            "Epoch 329/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5985 - acc: 0.6742 - val_loss: 0.6078 - val_acc: 0.6906\n",
            "Epoch 330/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5965 - acc: 0.6880 - val_loss: 0.6013 - val_acc: 0.6869\n",
            "Epoch 331/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5965 - acc: 0.6835 - val_loss: 0.6122 - val_acc: 0.6757\n",
            "Epoch 332/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5971 - acc: 0.6786 - val_loss: 0.6048 - val_acc: 0.6906\n",
            "Epoch 333/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5964 - acc: 0.6817 - val_loss: 0.6021 - val_acc: 0.6794\n",
            "Epoch 334/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5983 - acc: 0.6835 - val_loss: 0.6082 - val_acc: 0.6906\n",
            "Epoch 335/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5952 - acc: 0.6854 - val_loss: 0.6014 - val_acc: 0.6813\n",
            "Epoch 336/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5955 - acc: 0.6798 - val_loss: 0.6011 - val_acc: 0.6887\n",
            "Epoch 337/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5949 - acc: 0.6912 - val_loss: 0.6077 - val_acc: 0.6747\n",
            "Epoch 338/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5960 - acc: 0.6814 - val_loss: 0.6029 - val_acc: 0.6775\n",
            "Epoch 339/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5936 - acc: 0.6835 - val_loss: 0.6011 - val_acc: 0.6990\n",
            "Epoch 340/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5966 - acc: 0.6868 - val_loss: 0.6028 - val_acc: 0.6822\n",
            "Epoch 341/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5958 - acc: 0.6814 - val_loss: 0.6041 - val_acc: 0.6869\n",
            "Epoch 342/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5965 - acc: 0.6884 - val_loss: 0.6058 - val_acc: 0.6682\n",
            "Epoch 343/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5959 - acc: 0.6784 - val_loss: 0.5992 - val_acc: 0.6878\n",
            "Epoch 344/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5949 - acc: 0.6779 - val_loss: 0.6017 - val_acc: 0.6822\n",
            "Epoch 345/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5963 - acc: 0.6847 - val_loss: 0.6011 - val_acc: 0.6747\n",
            "Epoch 346/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5964 - acc: 0.6744 - val_loss: 0.6019 - val_acc: 0.6915\n",
            "Epoch 347/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5936 - acc: 0.6835 - val_loss: 0.6039 - val_acc: 0.6831\n",
            "Epoch 348/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5961 - acc: 0.6929 - val_loss: 0.6009 - val_acc: 0.6841\n",
            "Epoch 349/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5931 - acc: 0.6786 - val_loss: 0.6078 - val_acc: 0.6813\n",
            "Epoch 350/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5951 - acc: 0.6800 - val_loss: 0.6003 - val_acc: 0.6897\n",
            "Epoch 351/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5921 - acc: 0.6945 - val_loss: 0.6016 - val_acc: 0.6747\n",
            "Epoch 352/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5930 - acc: 0.6754 - val_loss: 0.6010 - val_acc: 0.6897\n",
            "Epoch 353/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5918 - acc: 0.6868 - val_loss: 0.5972 - val_acc: 0.6943\n",
            "Epoch 354/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5954 - acc: 0.6856 - val_loss: 0.6134 - val_acc: 0.6729\n",
            "Epoch 355/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5950 - acc: 0.6765 - val_loss: 0.5966 - val_acc: 0.6999\n",
            "Epoch 356/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5930 - acc: 0.6800 - val_loss: 0.5954 - val_acc: 0.6971\n",
            "Epoch 357/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5913 - acc: 0.6922 - val_loss: 0.6054 - val_acc: 0.6440\n",
            "Epoch 358/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5920 - acc: 0.6854 - val_loss: 0.6021 - val_acc: 0.6701\n",
            "Epoch 359/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5927 - acc: 0.6856 - val_loss: 0.5980 - val_acc: 0.6962\n",
            "Epoch 360/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5912 - acc: 0.6826 - val_loss: 0.6037 - val_acc: 0.6822\n",
            "Epoch 361/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5951 - acc: 0.6714 - val_loss: 0.5982 - val_acc: 0.6850\n",
            "Epoch 362/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5912 - acc: 0.6908 - val_loss: 0.6086 - val_acc: 0.6710\n",
            "Epoch 363/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5920 - acc: 0.6824 - val_loss: 0.5980 - val_acc: 0.6887\n",
            "Epoch 364/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5893 - acc: 0.6989 - val_loss: 0.5961 - val_acc: 0.6841\n",
            "Epoch 365/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5914 - acc: 0.6849 - val_loss: 0.5992 - val_acc: 0.6934\n",
            "Epoch 366/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5907 - acc: 0.6915 - val_loss: 0.5970 - val_acc: 0.6859\n",
            "Epoch 367/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5896 - acc: 0.6803 - val_loss: 0.6131 - val_acc: 0.6850\n",
            "Epoch 368/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5921 - acc: 0.6880 - val_loss: 0.5975 - val_acc: 0.6887\n",
            "Epoch 369/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5891 - acc: 0.6870 - val_loss: 0.5952 - val_acc: 0.6943\n",
            "Epoch 370/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5909 - acc: 0.6870 - val_loss: 0.6037 - val_acc: 0.6766\n",
            "Epoch 371/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5919 - acc: 0.6761 - val_loss: 0.5960 - val_acc: 0.7158\n",
            "Epoch 372/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5885 - acc: 0.6989 - val_loss: 0.5962 - val_acc: 0.6729\n",
            "Epoch 373/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5892 - acc: 0.6889 - val_loss: 0.5985 - val_acc: 0.6822\n",
            "Epoch 374/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5888 - acc: 0.6910 - val_loss: 0.5945 - val_acc: 0.6943\n",
            "Epoch 375/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5904 - acc: 0.6882 - val_loss: 0.5999 - val_acc: 0.6990\n",
            "Epoch 376/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5909 - acc: 0.6786 - val_loss: 0.5944 - val_acc: 0.7055\n",
            "Epoch 377/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5916 - acc: 0.6819 - val_loss: 0.6037 - val_acc: 0.6841\n",
            "Epoch 378/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5937 - acc: 0.6884 - val_loss: 0.6031 - val_acc: 0.6813\n",
            "Epoch 379/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5928 - acc: 0.6707 - val_loss: 0.5969 - val_acc: 0.6710\n",
            "Epoch 380/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5899 - acc: 0.6910 - val_loss: 0.5953 - val_acc: 0.6803\n",
            "Epoch 381/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5889 - acc: 0.6945 - val_loss: 0.5988 - val_acc: 0.6729\n",
            "Epoch 382/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5888 - acc: 0.6840 - val_loss: 0.5997 - val_acc: 0.6990\n",
            "Epoch 383/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5874 - acc: 0.6898 - val_loss: 0.5996 - val_acc: 0.6626\n",
            "Epoch 384/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5864 - acc: 0.6854 - val_loss: 0.5952 - val_acc: 0.7036\n",
            "Epoch 385/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5892 - acc: 0.6838 - val_loss: 0.5929 - val_acc: 0.6785\n",
            "Epoch 386/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5875 - acc: 0.6842 - val_loss: 0.5914 - val_acc: 0.7064\n",
            "Epoch 387/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5859 - acc: 0.6940 - val_loss: 0.5940 - val_acc: 0.6943\n",
            "Epoch 388/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5885 - acc: 0.6973 - val_loss: 0.5921 - val_acc: 0.6841\n",
            "Epoch 389/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5906 - acc: 0.6817 - val_loss: 0.5958 - val_acc: 0.6859\n",
            "Epoch 390/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5874 - acc: 0.6894 - val_loss: 0.5986 - val_acc: 0.6878\n",
            "Epoch 391/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5853 - acc: 0.6919 - val_loss: 0.5954 - val_acc: 0.6971\n",
            "Epoch 392/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5853 - acc: 0.6959 - val_loss: 0.5900 - val_acc: 0.7083\n",
            "Epoch 393/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5863 - acc: 0.6894 - val_loss: 0.5920 - val_acc: 0.6990\n",
            "Epoch 394/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5858 - acc: 0.6973 - val_loss: 0.5943 - val_acc: 0.6692\n",
            "Epoch 395/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5870 - acc: 0.6933 - val_loss: 0.5913 - val_acc: 0.6934\n",
            "Epoch 396/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5839 - acc: 0.6922 - val_loss: 0.5916 - val_acc: 0.6841\n",
            "Epoch 397/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5834 - acc: 0.6971 - val_loss: 0.5925 - val_acc: 0.6990\n",
            "Epoch 398/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5850 - acc: 0.6861 - val_loss: 0.5930 - val_acc: 0.7008\n",
            "Epoch 399/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5873 - acc: 0.6964 - val_loss: 0.5955 - val_acc: 0.6813\n",
            "Epoch 400/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5885 - acc: 0.6719 - val_loss: 0.5927 - val_acc: 0.6887\n",
            "Epoch 401/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5867 - acc: 0.6891 - val_loss: 0.5900 - val_acc: 0.7148\n",
            "Epoch 402/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5835 - acc: 0.6985 - val_loss: 0.5918 - val_acc: 0.6952\n",
            "Epoch 403/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5841 - acc: 0.6943 - val_loss: 0.5909 - val_acc: 0.6869\n",
            "Epoch 404/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5834 - acc: 0.7008 - val_loss: 0.5940 - val_acc: 0.6775\n",
            "Epoch 405/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5833 - acc: 0.6982 - val_loss: 0.5891 - val_acc: 0.6878\n",
            "Epoch 406/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5837 - acc: 0.6938 - val_loss: 0.5957 - val_acc: 0.7008\n",
            "Epoch 407/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5858 - acc: 0.6985 - val_loss: 0.5980 - val_acc: 0.6738\n",
            "Epoch 408/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5883 - acc: 0.6926 - val_loss: 0.5946 - val_acc: 0.7213\n",
            "Epoch 409/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5817 - acc: 0.7078 - val_loss: 0.5900 - val_acc: 0.6906\n",
            "Epoch 410/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5822 - acc: 0.6926 - val_loss: 0.5888 - val_acc: 0.7083\n",
            "Epoch 411/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5842 - acc: 0.6861 - val_loss: 0.5991 - val_acc: 0.6850\n",
            "Epoch 412/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5835 - acc: 0.6975 - val_loss: 0.5964 - val_acc: 0.6719\n",
            "Epoch 413/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5846 - acc: 0.6917 - val_loss: 0.5869 - val_acc: 0.6934\n",
            "Epoch 414/500\n",
            "34/34 [==============================] - 1s 16ms/step - loss: 0.5849 - acc: 0.6926 - val_loss: 0.5882 - val_acc: 0.6962\n",
            "Epoch 415/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5818 - acc: 0.7066 - val_loss: 0.5940 - val_acc: 0.6943\n",
            "Epoch 416/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5842 - acc: 0.6856 - val_loss: 0.5866 - val_acc: 0.7176\n",
            "Epoch 417/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5848 - acc: 0.6873 - val_loss: 0.5901 - val_acc: 0.6887\n",
            "Epoch 418/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5843 - acc: 0.6908 - val_loss: 0.5887 - val_acc: 0.6999\n",
            "Epoch 419/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5800 - acc: 0.6971 - val_loss: 0.5890 - val_acc: 0.6878\n",
            "Epoch 420/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5835 - acc: 0.6966 - val_loss: 0.5885 - val_acc: 0.6952\n",
            "Epoch 421/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5810 - acc: 0.6943 - val_loss: 0.5887 - val_acc: 0.6934\n",
            "Epoch 422/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5812 - acc: 0.6978 - val_loss: 0.5875 - val_acc: 0.6869\n",
            "Epoch 423/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5809 - acc: 0.6917 - val_loss: 0.5858 - val_acc: 0.6999\n",
            "Epoch 424/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5799 - acc: 0.7041 - val_loss: 0.5970 - val_acc: 0.6831\n",
            "Epoch 425/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5823 - acc: 0.6931 - val_loss: 0.5858 - val_acc: 0.7167\n",
            "Epoch 426/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5796 - acc: 0.7066 - val_loss: 0.5885 - val_acc: 0.6841\n",
            "Epoch 427/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5788 - acc: 0.7099 - val_loss: 0.5892 - val_acc: 0.6887\n",
            "Epoch 428/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5794 - acc: 0.7022 - val_loss: 0.5842 - val_acc: 0.7213\n",
            "Epoch 429/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5794 - acc: 0.7071 - val_loss: 0.5867 - val_acc: 0.7148\n",
            "Epoch 430/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5791 - acc: 0.6987 - val_loss: 0.5857 - val_acc: 0.7111\n",
            "Epoch 431/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5784 - acc: 0.6980 - val_loss: 0.5849 - val_acc: 0.6962\n",
            "Epoch 432/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5807 - acc: 0.7008 - val_loss: 0.5871 - val_acc: 0.6869\n",
            "Epoch 433/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5792 - acc: 0.6987 - val_loss: 0.5865 - val_acc: 0.7092\n",
            "Epoch 434/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5821 - acc: 0.6989 - val_loss: 0.5893 - val_acc: 0.6831\n",
            "Epoch 435/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5799 - acc: 0.6957 - val_loss: 0.5842 - val_acc: 0.7092\n",
            "Epoch 436/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5776 - acc: 0.7013 - val_loss: 0.5886 - val_acc: 0.7120\n",
            "Epoch 437/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5786 - acc: 0.7083 - val_loss: 0.5880 - val_acc: 0.6906\n",
            "Epoch 438/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5776 - acc: 0.7008 - val_loss: 0.5898 - val_acc: 0.7008\n",
            "Epoch 439/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5771 - acc: 0.6996 - val_loss: 0.5923 - val_acc: 0.7195\n",
            "Epoch 440/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5769 - acc: 0.7118 - val_loss: 0.5868 - val_acc: 0.6943\n",
            "Epoch 441/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5795 - acc: 0.6947 - val_loss: 0.5846 - val_acc: 0.7036\n",
            "Epoch 442/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5769 - acc: 0.7111 - val_loss: 0.5914 - val_acc: 0.7092\n",
            "Epoch 443/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5783 - acc: 0.7015 - val_loss: 0.5826 - val_acc: 0.7055\n",
            "Epoch 444/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5772 - acc: 0.7097 - val_loss: 0.5848 - val_acc: 0.7213\n",
            "Epoch 445/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5784 - acc: 0.7069 - val_loss: 0.5866 - val_acc: 0.6962\n",
            "Epoch 446/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5764 - acc: 0.7090 - val_loss: 0.5857 - val_acc: 0.7064\n",
            "Epoch 447/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5771 - acc: 0.6910 - val_loss: 0.5909 - val_acc: 0.6738\n",
            "Epoch 448/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5769 - acc: 0.6987 - val_loss: 0.5816 - val_acc: 0.7092\n",
            "Epoch 449/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5777 - acc: 0.7066 - val_loss: 0.5901 - val_acc: 0.6869\n",
            "Epoch 450/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5772 - acc: 0.6966 - val_loss: 0.5817 - val_acc: 0.7036\n",
            "Epoch 451/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5735 - acc: 0.7120 - val_loss: 0.5840 - val_acc: 0.6803\n",
            "Epoch 452/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5765 - acc: 0.7090 - val_loss: 0.5851 - val_acc: 0.6906\n",
            "Epoch 453/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5762 - acc: 0.7022 - val_loss: 0.5815 - val_acc: 0.7130\n",
            "Epoch 454/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5767 - acc: 0.7031 - val_loss: 0.5877 - val_acc: 0.6813\n",
            "Epoch 455/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5755 - acc: 0.7094 - val_loss: 0.5803 - val_acc: 0.7018\n",
            "Epoch 456/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5741 - acc: 0.7078 - val_loss: 0.5799 - val_acc: 0.7092\n",
            "Epoch 457/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5750 - acc: 0.7115 - val_loss: 0.5838 - val_acc: 0.6831\n",
            "Epoch 458/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5746 - acc: 0.7024 - val_loss: 0.5817 - val_acc: 0.6990\n",
            "Epoch 459/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5747 - acc: 0.7041 - val_loss: 0.5819 - val_acc: 0.7158\n",
            "Epoch 460/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5750 - acc: 0.7071 - val_loss: 0.5811 - val_acc: 0.7111\n",
            "Epoch 461/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5739 - acc: 0.6973 - val_loss: 0.5825 - val_acc: 0.6878\n",
            "Epoch 462/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5744 - acc: 0.7094 - val_loss: 0.5782 - val_acc: 0.7064\n",
            "Epoch 463/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5803 - acc: 0.6994 - val_loss: 0.5837 - val_acc: 0.6850\n",
            "Epoch 464/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5741 - acc: 0.7036 - val_loss: 0.5839 - val_acc: 0.7055\n",
            "Epoch 465/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5776 - acc: 0.6903 - val_loss: 0.5883 - val_acc: 0.6794\n",
            "Epoch 466/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5756 - acc: 0.7101 - val_loss: 0.5785 - val_acc: 0.7185\n",
            "Epoch 467/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5767 - acc: 0.7064 - val_loss: 0.5921 - val_acc: 0.7139\n",
            "Epoch 468/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5756 - acc: 0.7097 - val_loss: 0.5798 - val_acc: 0.7046\n",
            "Epoch 469/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5737 - acc: 0.7108 - val_loss: 0.5899 - val_acc: 0.6934\n",
            "Epoch 470/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5740 - acc: 0.7017 - val_loss: 0.5825 - val_acc: 0.7102\n",
            "Epoch 471/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5724 - acc: 0.7125 - val_loss: 0.5832 - val_acc: 0.6962\n",
            "Epoch 472/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5754 - acc: 0.7003 - val_loss: 0.5826 - val_acc: 0.6850\n",
            "Epoch 473/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5756 - acc: 0.6982 - val_loss: 0.5887 - val_acc: 0.6915\n",
            "Epoch 474/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5707 - acc: 0.7043 - val_loss: 0.5837 - val_acc: 0.7120\n",
            "Epoch 475/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5741 - acc: 0.7106 - val_loss: 0.5806 - val_acc: 0.6952\n",
            "Epoch 476/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5727 - acc: 0.7010 - val_loss: 0.5782 - val_acc: 0.7204\n",
            "Epoch 477/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5702 - acc: 0.7134 - val_loss: 0.5799 - val_acc: 0.7176\n",
            "Epoch 478/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5697 - acc: 0.7129 - val_loss: 0.5822 - val_acc: 0.6980\n",
            "Epoch 479/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5715 - acc: 0.7048 - val_loss: 0.5758 - val_acc: 0.7130\n",
            "Epoch 480/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5727 - acc: 0.7052 - val_loss: 0.5761 - val_acc: 0.7195\n",
            "Epoch 481/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5712 - acc: 0.7085 - val_loss: 0.5821 - val_acc: 0.7139\n",
            "Epoch 482/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5693 - acc: 0.7104 - val_loss: 0.5784 - val_acc: 0.7046\n",
            "Epoch 483/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5695 - acc: 0.7157 - val_loss: 0.5774 - val_acc: 0.7148\n",
            "Epoch 484/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5699 - acc: 0.7120 - val_loss: 0.5768 - val_acc: 0.7008\n",
            "Epoch 485/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5701 - acc: 0.7104 - val_loss: 0.5814 - val_acc: 0.6785\n",
            "Epoch 486/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5733 - acc: 0.7057 - val_loss: 0.5758 - val_acc: 0.7130\n",
            "Epoch 487/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5704 - acc: 0.7085 - val_loss: 0.5764 - val_acc: 0.7046\n",
            "Epoch 488/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5680 - acc: 0.7148 - val_loss: 0.5756 - val_acc: 0.7111\n",
            "Epoch 489/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5680 - acc: 0.7108 - val_loss: 0.5829 - val_acc: 0.7111\n",
            "Epoch 490/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5694 - acc: 0.7066 - val_loss: 0.5769 - val_acc: 0.7176\n",
            "Epoch 491/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5692 - acc: 0.7143 - val_loss: 0.5780 - val_acc: 0.7213\n",
            "Epoch 492/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5673 - acc: 0.7141 - val_loss: 0.5784 - val_acc: 0.6980\n",
            "Epoch 493/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5697 - acc: 0.7106 - val_loss: 0.5810 - val_acc: 0.7027\n",
            "Epoch 494/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5760 - acc: 0.7006 - val_loss: 0.5807 - val_acc: 0.6962\n",
            "Epoch 495/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5690 - acc: 0.7097 - val_loss: 0.5786 - val_acc: 0.6962\n",
            "Epoch 496/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5705 - acc: 0.7104 - val_loss: 0.5812 - val_acc: 0.7111\n",
            "Epoch 497/500\n",
            "34/34 [==============================] - 1s 18ms/step - loss: 0.5675 - acc: 0.7106 - val_loss: 0.5881 - val_acc: 0.6813\n",
            "Epoch 498/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5702 - acc: 0.7066 - val_loss: 0.5732 - val_acc: 0.7120\n",
            "Epoch 499/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5702 - acc: 0.7181 - val_loss: 0.5803 - val_acc: 0.7148\n",
            "Epoch 500/500\n",
            "34/34 [==============================] - 1s 17ms/step - loss: 0.5662 - acc: 0.7150 - val_loss: 0.5784 - val_acc: 0.7036\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f965c4850>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrlIunXx-hR0"
      },
      "source": [
        "dnn_predict = model.predict(x_test)\n",
        "dnn_predict = round(pd.DataFrame((dnn_predict)))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "zi0mZ1XkIXjJ",
        "outputId": "79cfd8b2-557c-459d-d4c0-2f1581151696"
      },
      "source": [
        "#confusion matrix for DNN \n",
        "cm_rf = confusion_matrix(y_test, dnn_predict)\n",
        "print(cm_rf)\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.title('Confusion Matrix', size=16)\n",
        "sns.heatmap(cm_rf, annot=True, cmap='Blues')\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[330 190]\n",
            " [128 425]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1f83edf750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ3MEWdXCNUv"
      },
      "source": [
        "## The confusion matrix demonstartes that the DNN does produce a more suitable model which achieves slightly more accuracy and more balanced predictions. Overall, the DNN is more efficent than the other ML models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m83bd1QM5K7L",
        "outputId": "9ab420fb-1bf3-4e27-cbe8-841dbee64d32"
      },
      "source": [
        "#Forecast 5 obs \n",
        "Xnew, _ = make_blobs(n_samples=5, centers=5, n_features=6, random_state=1)\n",
        "x_new = np.concatenate((x_test, Xnew))\n",
        "ynew = model.predict(x_new)\n",
        "predictions = ynew[-5:]\n",
        "for i in range(len(predictions)):\n",
        "    print(\"Prediction = \", predictions[i])"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction =  [1.]\n",
            "Prediction =  [0.]\n",
            "Prediction =  [1.]\n",
            "Prediction =  [1.]\n",
            "Prediction =  [0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUqQAMjPAkTg"
      },
      "source": [
        "##It can be seen that none of the models can forcast/predict Apple's share price with a high level of accuracy. The DNN was potentially too complex for the problem presented to it, although I would have liked to explore a GRID search and improving performance with different optimizers, cost functions etc. It would have been very interesting to present a DNN or even a CNN with a more expansive dataset. "
      ]
    }
  ]
}